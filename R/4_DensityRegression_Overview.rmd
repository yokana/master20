---
title: "Density-on-scalar regression for class performence"
author: "Johannes Wagner"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: yes
    code_folding: hide
---
  
```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, message=F, warning=F, comment=NA, autodep=F, 
                      eval=T, cache.rebuild=F, cache=F, R.options=list(width=120), 
                      fig.width=8, fig.align = 'center', dev.args=list(bg = 'transparent'), dev='svglite')
```

```{r echo=FALSE}
library(tidyverse)
library(kdensity) # density estimation
library(FDboost) # for functional analysis
library(compositions) # for clr transformation
```

# Density-on-scalar estimation

## Prerequisites 

1. Load the dataset from chapter 1

```{r echo=TRUE}
load("/home/johannes/Documents/Projects/2020_Master_STAR/master20/data/main_data.RData")    
main_data <- main_data %>% select(perc_av_gk, perc_av_gk_sc, gkclasstype, gkschid, gktchid)
```

2. Comments

At this point we concentrate the analysis on student percentiles for average scores in kindergarten, with the following variables of interest:

- 5764 observations of students score percentiles: `perc_av_gk`
- 320 `classes`
- 79 `schools`
- 3 `classtypes` with levels *SMALL*, *REG* and *wAid*


### Data Review (Histograms)

A simple approach of visualizing group differences in distribution is to plot respective histograms. Following the histograms for percentiles given grade and class size are shown:

```{r echo=TRUE}
smallClassesGK <- main_data %>% 
  select(perc_av_gk, gkclasstype) %>% 
  filter(gkclasstype=="SMALL CLASS" & !is.na(perc_av_gk)) 
regClassesGK <- main_data %>% 
  select(perc_av_gk, gkclasstype) %>% 
  filter(gkclasstype=="REGULAR CLASS" & !is.na(perc_av_gk)) 
regAidClassesGK <- main_data %>% 
  select(perc_av_gk, gkclasstype) %>% 
  filter(gkclasstype=="REGULAR + AIDE CLASS" & !is.na(perc_av_gk)) 

# we want to filter the dataset for each class
groupList <- unique(main_data$gkclasstype)
```



```{r echo=TRUE}
par(mfrow = c(1,3))
h1 <- hist(smallClassesGK$perc_av_gk, plot=F, breaks = 20)
h1$counts <- h1$counts / sum(h1$counts)
plot(h1, freq=TRUE, ylab="Relative Frequency", ylim = c(0,0.10))
h2 <- hist(regClassesGK$perc_av_gk, plot = F, breaks = 20)
h2$counts <- h2$counts / sum(h2$counts)
plot(h2, freq=TRUE, ylab="Relative Frequency", ylim = c(0,0.10))
h3 <- hist(regAidClassesGK$perc_av_gk, plot = F, breaks = 20)
h3$counts <- h3$counts / sum(h3$counts)
plot(h3, freq=TRUE, ylab="Relative Frequency", ylim = c(0,0.10))

```



### Show group specific densities

```{r echo=TRUE}
# make a list to store all densities
SizeTypeDensities <- vector("list", length = length(groupList)-1)
# ignore first value of List since it is NA
for (i in 2:length(groupList)){
  temp <- main_data %>% 
    select(gkclasstype, perc_av_gk) %>% 
    filter(gkclasstype==groupList[i])
  SizeTypeDensities[[i-1]] <- kdensity(temp$perc_av_gk, start = "uniform", kernel = "beta_biased", support = c(0,1), na.rm = T, bw = 0.05)}  
par(mfrow = c(1,3))
plot(SizeTypeDensities[[2]], ylim = c(0.5,1.5), main = "Small Classes")
plot(SizeTypeDensities[[3]], ylim = c(0.5,1.5), main = "Regular Classes")
plot(SizeTypeDensities[[1]], ylim = c(0.5,1.5), main = "with Aid")

```

So far we only implemented a more or less arbitrary bandwidth which produced satisfying results. This should be replaced by a well founded procedure. 

A uniform start distribution is chosen due to the estimation procedure of percentiles.


## Density estimation

The first step is built on the theoretical assumptions that the observed student score percentiles are realizations of an underlying conditional density function. Or in other words we observe the conditional distributions of **class specific densities** (relative performance of students) in their discrete form (cp. Talska et al. 2018). We want to estimate a density function for each **group of interest** or partition of the observational space given the available covariates. For the most simple model at kindergarten level this would be $79*3=237$ densities.

One big advantage of the distributional perspective is that it enables the researcher to analyze the whole distribution of the response instead of being focus on single distributional parameters like the mean of a distribution (Talska 2018,p.67)

In addition, there are several parameters for kernel estimation that we need to tune carefully (see `?kdensity`):

- optimal bandwidth: **so far user supplied**
- parametric start: "uniform"
- kernel: "beta_biased" as the recommended bias-corrected kernel for use on unit interval

Estimation:

```{r echo=TRUE}
# split the data by school*classtype
# create a variable which combines both levels
main_data <- main_data %>% filter(!is.na(perc_av_gk)) %>% 
  mutate(school_by_type = interaction(gkschid,gkclasstype, sep=".", lex.order = T))

# create a vector of unique identifiers for each density sorted by school ID
# attributes(main_data$school_by_type) # factor with 237 levels
vecIdent <- sort(unique(main_data$school_by_type))

# make a list to store all densities (overall and school specific)
# we want to use school-specific densities for visualization, since they are removing variation between schools, which we are not interested in
densities <- vector("list", length = length(vecIdent))
densities_sc <- vector("list", length = length(vecIdent))
# ignore first value of List since it is NA
for (i in 1:length(vecIdent)){
  temp <- main_data %>% 
    select(school_by_type, perc_av_gk, perc_av_gk_sc) %>% 
    filter(school_by_type==vecIdent[i])
  densities[[i]] <- kdensity(temp$perc_av_gk, start = "uniform", kernel = "beta_biased", bw = 0.1)
  densities_sc[[i]] <- kdensity(temp$perc_av_gk, start = "uniform", kernel = "beta_biased", bw = 0.1)
  }  
```

To get a rough idea of the data, we plot the estimated densities by `classtype`:

```{r echo=TRUE}
## we can use a vector of classtypes for coloring the corresponding plot
colVec <- factor(str_sub(vecIdent, start = 8))
plot(densities_sc[[1]], ylim = c(0,6), col = colVec[1])
legend("topright", legend=c("SMALL", "REG", "wAID"), pch=16,col=unique(colVec))
for (i in 2:length(densities)){
  lines(densities[[i]], col=colVec[i])
}
```

At this point, we are not able to identify a clear picture. But we do see that extremely high densities are exclusively located in the lowest quantile of the percentile distribution.

## Covariates


Following, a first density-on-scalar estimation for the STAR data is represented.
So far the model only accounts for (well known) effects of assigned classtypes and only analyses students in kindergarten.

We use data that consists of a list of

- the densities (kernel estimation from chapter 3) for the percentiles for average score results for students in kindergarten evaluated at 100 equidistant points
- a vector of evaluation points that were used to retrieve observation points from the estimated densities
- a vector of classtypes with levels: "SMALL", "Reg", "wAid"
- a dummy vector indicating the assignment of a SMALL class
- a vector of school specific indicators with 79 levels


Create Covariates:

```{r echo=TRUE}
# create a list of classtypes as factor with 1=SMALL, 2=AID, 3=wAid for all densities
listObjClasstype <- main_data %>% 
  # automatically orders tibble by schoolID and clastype to correspond to order in classList
  group_by(gkschid, gkclasstype) %>% 
  summarise(.groups = "drop") %>% 
  select(gkclasstype) %>%
  filter(!is.na(gkclasstype)) %>%
  unlist(use.names = F) %>%
  factor(labels=c("SMALL", "Regular", "wAid"))

# create an addition Dummy Vector for classtype SMALL
dummyClasstype <- recode(listObjClasstype, "SMALL" = 1, "Regular" = 0, "wAid" = 0)
  
# create a list of school IDs for all estimated densities
listObjSchool <- main_data %>% 
  # automatically orders tibble by gktchid to correspond to order in classList
  group_by(gkschid, gkclasstype) %>% 
  summarise(.groups = "drop") %>% 
  select(gkschid) %>%
  filter(!is.na(gkschid)) %>%
  unlist(use.names = F) 

# comp_data_gender <- 
#   comp_data_gk %>% group_by(gktchid, gender) %>% 
#     summarise(n = n(), .groups="drop_last" ) %>% 
#       mutate(share_gender = n / sum(n)) %>% 
#         pivot_wider(id_cols =gktchid, names_from=gender, values_from= share_gender)

```

Get densities values at 100 equidistant evaluation points:

```{r echo=TRUE}
numPoints = 100
# make a matrix where each row is a density and each column is an evaluation point
denMatrix_1 <- matrix(data = NA, nrow = length(densities), ncol = numPoints)
# for school specific densities
denMatrix_2 <- matrix(data = NA, nrow = length(densities_sc), ncol = numPoints)

eqdist <- seq(0.001,0.999,length.out = numPoints)  

for (i in 1:length(densities)){
  denMatrix_1[i,] <- densities[[i]](eqdist)
  denMatrix_2[i,] <- densities_sc[[i]](eqdist)
}  

```

## Clr Transformation

Theoretically we must transform our densities, which are elements of an **Bayes Hilbert Space** into the usual space for functional analysis: **L2** (See Eva Maria Maier, p.14). After this transformation we are able to perform function-on-scalar regression: 
$$
\operatorname{clr}_{\lambda}\left[f_{\mu}\right]:=\log f_{\mu}-\frac{1}{\lambda(\mathcal{T})} \cdot \int_{\mathcal{T}} \log f_{\mu} \mathrm{d} \lambda
$$

Since we evaluate the densities along a vector of equidistant points this transformation corresponds to the discrete clr-transformation of compositional variables (See Boogart et al. 2013, p.41, Talska 2018, p. 70).

Since we want to avoid taking logarithms of value zero, we choose our equidistant points from the closed interval $[0.001,0.999]$.

To put it more simply, when we evaluate our densities at equidistant points we get "quasi" compositional data. Then we proceed within the compositional framework. Although this is the most straight forward framework, more complex estimation procedures are proposed (see Talska 2018).[is this correct??]

We can easily transform the vector of evaluation points by the `clr` command from Boogarts et al. `composition` R add on package. 


```{r echo=TRUE}
clrDenMatrix_1 <- clr(denMatrix_1)
clrDenMatrix_2 <- clr(denMatrix_2)
# after clr transformation the vector of values sums up to zero
```

We combine the dependent vectors and the covariates into a list object

```{r echo=TRUE}
# change covariates since 


# create a list with all variables for functional analysis  
star <- list(Den_gk = clrDenMatrix_1, 
             Den_gk_sc = clrDenMatrix_2,
             evPoints = eqdist,
             densityID = vecIdent,
             ClassType = listObjClasstype,
             ClassDummy = as.factor(dummyClasstype),
             School = listObjSchool) 

# save list as it is the core data for further analysis
setwd("~/Documents/Projects/2020_Master_STAR/master20/data")
save(star, file = "functionalData.RData")
```  


## Representation of clr-transformed densities

At first, we can plot all class-specific densities into one plot and for specific groups. There are a few extreme classes with extremely skewed distributions to their left tails. To make the plots more reliable we restrict the y-axis for the group specific plots on values between -20 and 15 for the centered log ratios.


```{r echo=TRUE}
# get the densities
depDens <- star$Den_gk_sc
evPoints <- star$evPoints
classType <- star$ClassType
plot(evPoints,as.double(depDens[1,]), type="l", ylim = c(-20,15), col = colVec[1])
legend("topright", legend=c("SMALL", "REG", "wAID"), pch=16,col=unique(colVec))
for (i in 2:nrow(depDens)){
  lines(evPoints,as.double(depDens[i,]), type="l", col=colVec[i])
}   
```

There are a few classes with a relative underrepresentation of students with very low percentile ranks and those are SMALL classes. **We expect them to influence the regression model quite a lot.** The same can be expected for the few (two) regular classes with strong underrepresentation within the top quantile. We can also see that the clr-transformation seems to represent the relational structure of the data much better then the original densities as seen above. 



```{r echo=TRUE}
par(mfrow = c(1,3))
subDepDens <- depDens[classType=="SMALL",]
plot(evPoints,as.double(subDepDens[1,]), type="l", ylim = c(-20,15), main="Small classes")
for (i in 2:nrow(subDepDens)){
  lines(evPoints,as.double(subDepDens[i,]), type="l", ylim = c(-20,15))
}    
subDepDens <- depDens[classType=="Regular",]
plot(evPoints,as.double(subDepDens[1,]), type="l", ylim = c(-20,15), main="Reg classes")
for (i in 2:nrow(subDepDens)){
  lines(evPoints,as.double(subDepDens[i,]), type="l", ylim = c(-20,15))
}   
subDepDens <- depDens[classType=="wAid",]
plot(evPoints,as.double(subDepDens[1,]), type="l", ylim = c(-20,15), main="with Aides")
for (i in 2:nrow(subDepDens)){
  lines(evPoints,as.double(subDepDens[i,]), type="l", ylim = c(-20,15))
}   
```


The amount of variation for regular classes is much smaller than for SMALL classes, but again this is probably heavily influenced by the extreme classes. It might be a good idea to check other loss functions (e.g. absolute error).


Those Plots are probably not very helpful though. **IGNORE**


## Model Specification 

Starting with the regression of (school-specific) class densities onto the classtype while controlling for schools, we estimate several possible Specifications for the regression problem going from simple to more complex models.  

$$
\begin{array}{l}
\operatorname{clr}\left[\hat{f}_{\text {school, classtype, grade=kindergarten }}\right]=\operatorname{clr}\left[\beta_{0}\right]+\operatorname{clr}\left[\beta_{\text {classtype }}\right]+\operatorname{clr}\left[\varepsilon_{\text {region, child group, year }}\right]
\end{array}
$$

## Regression: Linear Effects of ClassType

We use the *component-wise gradient boosting algorithm* (cp. Eva Maria Maier) implemented in the `FDboost` add on package (Brockhaus, 2020) to calculate linear effects for the three factors of the `classtype` covariate. 

There is a set of parameters that need clarification:

- penalization parameter
  + degrees of freedom
    + default is 4 (Hofner 2014, p.13)
  + lambda (is computed automatically if df is specified) (default: $trace(2S âˆ’ S T S)$)
- type of effect (linear or smooth) / specification of base learners (Overview: Brockhaus 2020,p.7, 26)
- offset values (default: `offset=NULL`)
- optimal stopping iteration (use `cvrisk`)
- loss function / `family`
- step length

We use `bols` to estimate the model with ridge-penalized linear effects for classtype (Brockhaus 2020,p.17, Hofner 2014, p.12ff). `bols` can be used for continues and categorical covariates.

Since the covariate effect is represented as a Kronecker product of two marginal bases (Brockhaus 2020), we combine a linear base learner (`bols`) with a smooth effect estimated by P-splines over all evaluation points of the dependent densities (`bbs`) (Hofner 2014, p. 16). 
```{r echo=TRUE}
mod0 <- FDboost(Den_gk ~ 1,
               timeformula = ~ bbs(evPoints, df = 4),
               offset_control = o_control(k_min = 40), # doesnt change our curve
               offset = "scalar", data = star)
# this model estimates the "mean" curve of our response densities
# we know that this should result in a uniform distribution 
# does this mean we somehow underestimate values around the boundaries trough our 
# density estimation?

mod1 <- FDboost(Den_gk ~ 1 + bols(ClassType, intercept = T, df = 4),
               timeformula = ~ bbs(evPoints),
               offset = "scalar", data = star)
# do we need to center classType? (bols vs bolsc), vgl. Hofner 2014,p.12)
# Intercept should be random at school level??
# mod1b <- FDboost(Den_gk ~ 1 + bolsc(ClassType, intercept = T, df = 4),
#                timeformula = ~ bbs(evPoints),
#                offset = "scalar", data = star)
mod1c <- FDboost(Den_gk ~ 1 + bols(ClassDummy, df = 4),
               timeformula = ~ bbs(evPoints),
               offset = "scalar", data = star)
mod1d <- FDboost(Den_gk ~ 1 + bols(ClassDummy, df = 4) +  bols(classType, df = 4) ,
               timeformula = ~ bbs(evPoints),
               offset = "scalar", data = star)
# coefficients are evaluated at 24 points, which can be retrieved



plot(mod1, which = 1, main = "Intercept model", ylim = c(-1,1))
```

The Intercept model can be interpreted as the mean (clr)-density (which is not a density since values are still elements of L2 and need to be backtransformed into the Bayes Hilbert Space). The result corresponds to our expectation given the estimation procedure of the student percentiles. 

```{r echo=TRUE}
plot(mod1, which = 2, main = "Linear effects of classtype", ylim = c(-1,1))
legend("topright", legend=c("wAid", "REG", "SMALL"), pch=16,col=unique(colVec))  

```

The clr-trsanformed coefficients of the three classtypes shows that SMALL classes have lower (clr)-densities for [.,0.5]-percentiles compared to the other two classtypes.



```{r echo=TRUE}
# plot(mod1c, which = 2, main = "Linear effects of indicator for SMALL class", ylim = c(-1,1))
# legend("bottomright", legend=c("OTHER", "SMALL"), pch=16,col=unique(colVec))  

```



```{r echo=TRUE}
# convert the coefficients from L2 into Bayes Space
# origCoeff <- mod1$coef(which = 2)
# origCoeff2 <- mod1$coef()$`"bols(ClassType, intercept = T, df = 4) %O% bbs(evPoints)"`
# transfCoeff <- clrInv(origCoeff)

```
